{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML2019 ASSIGNMENT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper proposes two algorithms for the approximate nearest neighbor problem in high-dimensional spaces. For the data set of size $R^d$, the algorithms require preprocessing cost polynomial in $n$ and $d$, while achieving a sublinear query time. The paper also gives applications to information retrieval, pattern recognition, dynamic closest-pairs, and fast clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __nearest neighbor(NN)__ problem is following: Given a set $P$ of $n$ points in a metric space defined over a set $X$ with distance function $D$, preprocess $P$ to efficiently answer queries for finding the point in $P$ closest to a query point $q \\in X$.\n",
    "For low-dimensional cases (that is, the number of features in a sample is not large), linear search is a simple and efficient algorithm for NN problem. But for a large number of high-dimensional data sets, if you use linear search, it will consume a lot of time cost, this problem is called \"curse of dimensionality\".\n",
    "For this high-dimensional case, the known high-dimensional NNS algorithm can encounter one of two situations: (a) the preprocessing time is low, but the query time is linear through the number of points $n$ and dimension $d$, or (b) the query time is sublinear in $n$ and polynomial in $d$, but with severely exponential prprocessing cost $n^d$.\n",
    "Researchers have taken many approaches to solve the problem of NNS in high dimensions. For example, many data structures suitable for NN (_k-d_ trees, *R*-trees, etc.). However, although some methods performed well in 2-3 dimensions, in the high-dimensional space, they all performed poorly in the worst case and typical cases. For example, the first proposed algorithm for dealing with NN problem, which is with query time $O(2^dlog n)$ and preprocessing cost $O(n^{2^{d+1}})$. Researchers tried to reduce the consumption of preprocessing or query based on this algorithm. Unfortunately, none of these methods can avoid the exponential level of time spent on preprocessing and query at the same time.\n",
    "In order to propose a practical algorithm for NN search in high dimensional space, authors thought they have to relax their algorithmic guarantee. Roughly speaking, they will allow the query to return \"close enough\" instead of the closest point. This problem is called __approximate nearest neighbor(ANN)__. For the ANN algorithm, the situation is slightly better. The cost of pre-processing or query is gradually reduced. The researchers proposed an algorithm for retrieving all points within distance $r$ of the query $q$ in Hamming space. But these algorithms are exponential for the time spent on any distance. After that, a scheme was proposed, which is for binary data chosen uniformly at random. This scheme can retrieves all points for distance $r$ of $q$ with query time $O(dn^{r/d})$ and preprocessing time $O(dn^{1+r/d})$.\n",
    "This paper finally came up with three propositions, namely:\n",
    "\n",
    "__Proposition 1__ _For $\\epsilon$ >0,there is an algorithm for $\\epsilon-NNS$ in $\\Re^d$ under the $l_p$ norm for $p \\in [1,2]$ which uses $Õ(n^{1+1/(1+\\epsilon)}+dn)$ preprocessing and require $Õ(dn^{1/(1+\\epsilon)})$ query time._ \n",
    "\n",
    "__Proposition 2__ _For $0<\\epsilon<1$, there is an algorithm for $\\epsilon-NNS$ in $\\Re^d$ under any $l_p$ norm which uses $Õ(n)\\times O(1/\\epsilon)^d$ preprocessing and requires $Õ(d)$ querytime._\n",
    "\n",
    "__Proposition 3__ _For any $\\epsilon>0$, there is an algorithm for $\\epsilon-NNS$ in $\\Re^d$ under the $l_p$ norm for $p \\in [1,2]$ which uses $p\\in [1,2]$ which uses $(nd)^{O(1)}$ preprocessing and requires $Õ(d)$ query time._\n",
    "\n",
    "These results are obtained through a key idea, namely reducing e-NNS to problems of point location in equal balls(PLEB). PLEB is generally given n balls with radius r, for each query q, returns YES if it is in ball, else return NO. It has been observed that PLEB can be reduced to NNS, with the same dependency on preprocessing and query.It was proved that it is also available to reduce from NNS to PLEB, with a small overhead in preprocessing and query costs. This reduction needs a special data structure called ring-cover tree, which is based on rings and covers. The way to build this tree is recursion. In this data structure, for any point in the $P$ set, we can either find a ring-separator or a cover. Therefore, we can divide $P$ into multiple subsets. This decomposition allows us to quickly lock into one of the subsets when searching for $P$ sets, thereby increasing efficiency.\n",
    "\n",
    "There is also a reduction from $\\epsilon-NN$ to $\\epsilon-PLEB$, $r-PLEB$ is like a decision problem for $\\epsilon-NN.\n",
    "A simple implementation steps can be divided into the following steps:\n",
    "\n",
    "1) Set $R$ be the proportion between largest distance and smallest distance of 2 points.  \n",
    "2) Define $l=\\{(1+\\epsilon)^0,(1+\\epsilon)^1,...,R\\}$.  \n",
    "3) Generate $l-PLEB$ instances(balls) for each $l$.  \n",
    "4) For given query $q$, using binary search to find the smallest $l$ which there exists an $i$ such that $q\\in B^l_i$ and return $p_i$.\n",
    "\n",
    "The second technology is __locality-Sensitive Hashing__. The key idea is that similar items are more likely to collide. This technique increases the chance of mapping two data points to the same value as their distance $f(x,y)$ decreases. Through such a mapping, we can find adjacent data points in low-dimensional data space, avoiding spending too much time in high-dimensional data space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## innovation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
