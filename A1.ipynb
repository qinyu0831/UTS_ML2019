{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/qinyu0831/UTS_ML2019_ID13023662/blob/master/A1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Report on \"Approximate nearest neighbors: towards removing the curse of dimensionality\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __nearest neighbor(NN)__ problem is following: Given a set $P$ of $n$ points in a metric space defined over a set $X$ with distance function $D$, preprocess $P$ to efficiently answer queries for finding the point in $P$ closest to a query point $q \\in X$. \n",
    "\n",
    "To make it easy to understand, it can be summarized as: For a set $P$ of points in $R^d$ (or any metric space), given a query point $q$, find the point $p*$ in $P$ which is closest to $q$.\n",
    "\n",
    "<img src=\"NNP.jpg\">\n",
    "\n",
    "In 1999, after decades of hard work, solutions for NN problem in a low dimension has been satisfactory.The authors want to give a better solution for nearest neighbor problem under high-dimensional conditions. In this paper, they gave two algorithms to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to propose a practical algorithm for NN search in high dimensional space, authors thought they have to relax their algorithmic guarantee. Roughly speaking, they will allow the query to return \"close enough\" instead of the closest point. This problem is called __approximate nearest neighbor(ANN)__.This paper finally proposed two algorithms to solve the problem.\n",
    "\n",
    "These results are obtained through a key idea, namely reducing e-NNS problem to __point location in equal balls(PLEB)__. PLEB is generally given n balls with radius r, for each query q, returns YES if it is in ball, else return NO. The authors reduced NNS to PLEB. This reduction needs a special data structure called ring-cover tree, which is based on rings and covers. The way to build this tree is recursion. In this data structure, we can divide $P$ into multiple subsets. This decomposition allows us to quickly lock into one of the subsets when searching for $P$ sets, thereby increasing efficiency.\n",
    "\n",
    "There is also a reduction from $\\epsilon-NN$ to $l-PLEB$, $l-PLEB$ is like a decision problem for $\\epsilon-NN$.\n",
    "A simple implementation steps are:\n",
    "\n",
    "1) Set $R$ be the proportion between largest distance and smallest distance of 2 points.  \n",
    "\n",
    "2) Define $l=\\{(1+\\epsilon)^0,(1+\\epsilon)^1,...,R\\}$.  \n",
    "\n",
    "3) Generate $l-PLEB$ instances(balls) for each $l$.  \n",
    "\n",
    "4) For given query $q$, using binary search to find the smallest $l$ has an $i$ such that $q\\in B^l_i$ and return $p_i$.\n",
    "\n",
    "<img src=\"pleb.jpg\">\n",
    "\n",
    "Two techniques are proposed in the paper to deal with problem of $\\epsilon-PLEB$. The first method is __Bucketing Method__. The implementation steps are as follows:  \n",
    "\n",
    "1) Apply a grid of spacing $s=\\epsilon\\sqrt{d}$ so that every ball is covered by cubes.  \n",
    "\n",
    "2) Use hash table to store elements  \n",
    "\n",
    "   -Key: cube ; Value: a ball it covers  \n",
    "   \n",
    "3) For query $q$, compute the grid cell contains $q$ and check if it is stored in the hash table.\n",
    "\n",
    "<img src=\"bucket.jpg\">\n",
    "\n",
    "The second technology is __locality-Sensitive Hashing__.The core idea of this method is 'similar items are more likely to collide'. That is to say,as the distance $f(x,y)$ between two data points decreases, these two data points are more likely to map to the same value. Through such a mapping, we can find adjacent data points in low dimensions while avoiding spending too much time in high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innovation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the low-dimensional cases (that is, the number of features in a sample is not large), linear search is a simple and efficient algorithm for NN problem. But for a large number of high-dimensional data sets, if you use linear search, it will consume a lot of time cost, this problem is called \"curse of dimensionality\".\n",
    "\n",
    "For the high-dimensional case, the known high-dimensional NNS algorithm can not gave a satisfactory solution.\n",
    "\n",
    "Researchers have taken many approaches to solve the problem of NNS in high dimensions. For example, many data structures suitable for NN (_k-d_ trees, *R*-trees, etc.). However, although some methods performed well in low dimensions,in the high-dimensional space, they all performed poorly in some cases. Researchers tried to reduce the consumption of preprocessing or query based on this algorithm. Unfortunately, none of these methods can avoid the exponential level of time spent on preprocessing and query at the same time.\n",
    "\n",
    "The NNS algorithm is very important for a variety of applications, such as data compression, pattern recognition, information retrieval, and more. These applications require a similarity search. But as the number of features (ie, dimensions) of the object increases (from tens to thousands), the computation time is usually increased in polynomial or exponential manner. Some dimensionality reduction technologies such as latent semantic indexing(LSI) can reduce the dimensions of thousands to hundreds.\n",
    "\n",
    "In order to better solve the problem of NNS algorithm in high dimension, Indyk and Motwani(authors) propose an approximate method: Find a point $p$ approximately to $q$ instead of closest to $q$, which is the __approximate nearest neighbor problem__.\n",
    "\n",
    "Indyk and Motawani found that s-NNS problem can be reduced to __Point Location In Equal Balls(PLEB)__ problem , which is based on a data structure named Ring-cover trees.\n",
    "\n",
    "They gave two algorithms to solve the PLEB problem, __Bucketing Method__ and __Locality-Sensitive Hashing__ respectively.\n",
    "\n",
    "The authors' idea of converting the NNS problem to ANN is very creative and the research method reducing one problem to another problem is practical and worth learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the technical quality of the paper is very high. Through the use of mathematical equations, proofs and pseudocode, the origin of the NN dimension problem and the solution to the ANN problem are explained in detail. Not only can the reader understand the principles of the ANN, but also allow researchers to continue research on this basis. There are many applications listed in next section.\n",
    "\n",
    "The paper also has some shortcomings. For example, the lack of actual experimental display, such as process, results, etc., the author only directly gives the results of the experiment to prove the reliability of some algorithms or techniques. Unable to obtain specific information about the experiment, which will have a bad impact on the reliability of the paper. Furthermore, for $0 < \\epsilon < 1$, because of exponent depends on $1 / \\epsilon$, the result of the bucketing method algorithm is a purely theoretical result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application and X-Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper, the authors mentioned the applications of ANN search,such as information retrieval, pattern recognition, etc.\n",
    "\n",
    "The author concludes from an important inference in Locality-Sensitive Hashing that for data points $p,q\\in H^d$, their approximate measurement distance can be seen as a dot product $p\\cdot q$. It is a kind of measurement standard for information retrieval. This technique can be applied to compare the similarities between two documents. When a document is represented by a vector, a dot product can be used to calculate the distance between documents. This is a very useful application that can be used to build repetitive rate query systems and more.\n",
    "\n",
    "Dot product can also be used to solve the problem of the largest common point set. One of the its applications is image retrieval. Image retrieval is usually divided into two types(text-based or content-based) according to the way of describing image content. Today, as image data grows rapidly, text-based image retrieval has been eliminated. Obviously, getting the largest set of common points is a great help for content-based image retrieval.\n",
    "\n",
    "Approximate closet-pair queries can be easily accomplished by copying the PLEB algorithm. The scheme in paper can solve the dynamic closet-pair problem in sublinear time.This is a very powerful feature, because dynamic additions and deletions are a basic requirement in practical applications. \n",
    "\n",
    "The LSH algorithm and its variants are widely used today. They are successfully used to solve computational problems. In the new literature by Indyk and Andoni (2008), they developed the LSH family for the Euclidean distances.\n",
    "\n",
    "ANN also has many excellent libraries.\n",
    "\n",
    "ANNOY(Approximate Nearest Neighbor Oh Yeah) is an open source library. It is considered to be one of the best ANN libraries. In the recent version of ANNOY, it constructs multiple hierarchical 2-means trees. In each iteration, two centers are formed by clustering the subset of samples. These two centers define a partition hyperplane that is equidistant from the center. The data points are then divided into two subtrees by a hyperplane, and the algorithm recursively indexes on each subtree. The search process is performed by the travel tree nodes of multiple random projection trees. ANNOY has been used in the recommendation engine of spotify.com.\n",
    "\n",
    "FLANN(Fast Library for Approximate Nearest Neighbors) is an automatic nearest neighbor algorithm configuration method. It can select the most appropriate algorithm from the randomized kd-tree, hierarchical k-means tree and linear scan methods for the target data set, and can also specify the accuracy.\n",
    "\n",
    "In addition, ANN's libraries also includes Kgrapth, Nearpy, LsHash, etc.\n",
    "\n",
    "In Indyk, Motawani, Har-Peled's paper in 2010, they replaced PLEB with the closer term 'approximate near neighbor'. In addition, many algorithms are improved. The reduction for near to the nearest is much simpler and more efficient than before. It applies general metric spaces, and only needs near-linear number of ANN queries. New applications like 'The approximate minimum spanning tree' are also shown in the new paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the paper is clear and generally easy to read. I think that for questions such as PLEB, The Bucketing Method, or data structures like Ring-Cover Trees, it would be easier for readers to understand if they can add some images when interpreting.In addition, some special symbols or formulas may confuse the reader.It would be better if some mathematical expressions are accompanied by a clear explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indyk, P., & Motwani, R. 1998, May. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing (pp. 604-613). ACM.\n",
    "\n",
    "Har-Peled, S., Indyk, P., & Motwani, R. 2012. Approximate nearest neighbor: Towards removing the curse of dimensionality. Theory of computing, 8(1), 321-350.\n",
    "\n",
    "Andoni, A., & Indyk, P. 2008. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. Communications of the ACM, 51(1), 117."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
