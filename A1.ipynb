{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML2019 ASSIGNMENT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper proposes two algorithms for the approximate nearest neighbor problem in high-dimensional spaces. For the data set of size $R^d$, the algorithms require preprocessing cost polynomial in $n$ and $d$, while achieving a sublinear query time. The paper also gives applications to information retrieval, pattern recognition, dynamic closest-pairs, and fast clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __nearest neighbor(NN)__ problem is following: Given a set $P$ of $n$ points in a metric space defined over a set $X$ with distance function $D$, preprocess $P$ to efficiently answer queries for finding the point in $P$ closest to a query point $q \\in X$.\n",
    "For low-dimensional cases (that is, the number of features in a sample is not large), linear search is a simple and efficient algorithm for NN problem. But for a large number of high-dimensional data sets, if you use linear search, it will consume a lot of time cost, this problem is called \"curse of dimensionality\".\n",
    "For this high-dimensional case, the known high-dimensional NNS algorithm can encounter one of two situations: (a) the preprocessing time is low, but the query time is linear through the number of points $n$ and dimension $d$, or (b) the query time is sublinear in $n$ and polynomial in $d$, but with severely exponential prprocessing cost $n^d$.\n",
    "Researchers have taken many approaches to solve the problem of NNS in high dimensions. For example, many data structures suitable for NN (_k-d_ trees, *R*-trees, etc.). However, although some methods performed well in 2-3 dimensions, in the high-dimensional space, they all performed poorly in the worst case and typical cases. For example, the first proposed algorithm for dealing with NN problem, which is with query time $O(2^dlog n)$ and preprocessing cost $O(n^{2^{d+1}})$. Researchers tried to reduce the consumption of preprocessing or query based on this algorithm. Unfortunately, none of these methods can avoid the exponential level of time spent on preprocessing and query at the same time.\n",
    "In order to propose a practical algorithm for NN search in high dimensional space, authors thought they have to relax their algorithmic guarantee. Roughly speaking, they will allow the query to return \"close enough\" instead of the closest point. This problem is called __approximate nearest neighbor(ANN)__. For the ANN algorithm, the situation is slightly better. The cost of pre-processing or query is gradually reduced. The researchers proposed an algorithm for retrieving all points within distance $r$ of the query $q$ in Hamming space. But these algorithms are exponential for the time spent on any distance. After that, a scheme was proposed, which is for binary data chosen uniformly at random. This scheme can retrieves all points for distance $r$ of $q$ with query time $O(dn^{r/d})$ and preprocessing time $O(dn^{1+r/d})$.\n",
    "This paper finally came up with three propositions, namely:\n",
    "\n",
    "__Proposition 1__ _For $\\epsilon$ >0,there is an algorithm for $\\epsilon-NNS$ in $\\Re^d$ under the $l_p$ norm for $p \\in [1,2]$ which uses $Õ(n^{1+1/(1+\\epsilon)}+dn)$ preprocessing and require $Õ(dn^{1/(1+\\epsilon)})$ query time._ \n",
    "\n",
    "__Proposition 2__ _For $0<\\epsilon<1$, there is an algorithm for $\\epsilon-NNS$ in $\\Re^d$ under any $l_p$ norm which uses $Õ(n)\\times O(1/\\epsilon)^d$ preprocessing and requires $Õ(d)$ querytime._\n",
    "\n",
    "__Proposition 3__ _For any $\\epsilon>0$, there is an algorithm for $\\epsilon-NNS$ in $\\Re^d$ under the $l_p$ norm for $p \\in [1,2]$ which uses $p\\in [1,2]$ which uses $(nd)^{O(1)}$ preprocessing and requires $Õ(d)$ query time._\n",
    "\n",
    "These results are obtained through a key idea, namely reducing e-NNS to problems of point location in equal balls(PLEB). PLEB is generally given n balls with radius r, for each query q, returns YES if it is in ball, else return NO. It has been observed that PLEB can be reduced to NNS, with the same dependency on preprocessing and query.It was proved that it is also available to reduce from NNS to PLEB, with a small overhead in preprocessing and query costs. This reduction needs a special data structure called ring-cover tree, which is based on rings and covers. The way to build this tree is recursion. In this data structure, for any point in the $P$ set, we can either find a ring-separator or a cover. Therefore, we can divide $P$ into multiple subsets. This decomposition allows us to quickly lock into one of the subsets when searching for $P$ sets, thereby increasing efficiency.\n",
    "\n",
    "There is also a reduction from $\\epsilon-NN$ to $\\epsilon-PLEB$, $r-PLEB$ is like a decision problem for $\\epsilon-NN.\n",
    "A simple implementation steps can be divided into the following steps:\n",
    "\n",
    "1) Set $R$ be the proportion between largest distance and smallest distance of 2 points.  \n",
    "2) Define $l=\\{(1+\\epsilon)^0,(1+\\epsilon)^1,...,R\\}$.  \n",
    "3) Generate $l-PLEB$ instances(balls) for each $l$.  \n",
    "4) For given query $q$, using binary search to find the smallest $l$ which there exists an $i$ such that $q\\in B^l_i$ and return $p_i$.\n",
    "\n",
    "Two techniques are used for solving the $\\epsilon-PLEB$ problem. The first one is __Bucketing Method__. The implementation steps are as follows:  \n",
    "1) Apply a grid of spacing $s=\\epsilon\\sqrt{d}$ so that every ball is covered by cubes.  \n",
    "2) Use hash table to store elements  \n",
    "   -Key: cube ; Value: a ball it covers  \n",
    "3) For query $q$, compute the cell which contains $q$ and check if it is stored in the table.\n",
    "\n",
    "The second technology is __locality-Sensitive Hashing__. The key idea is that similar items are more likely to collide. This technique increases the chance of mapping two data points to the same value as their distance $f(x,y)$ decreases. Through such a mapping, we can find adjacent data points in low-dimensional data space, avoiding spending too much time in high-dimensional data space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innovation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NNS algorithm is very important for a variety of applications, such as data compression, pattern recognition, information retrieval, and more. These applications require a similarity search. But as the number of features (ie, dimensions) of the object increases (from tens to thousands), the computation time is usually increased in polynomial or exponential manner. Some dimensionality reduction technologies such as latent semantic indexing(LSI) can reduce the dimensions of thousands to hundreds.\n",
    "\n",
    "In order to better solve the problem of NNS algorithm in high dimension, Indyk and Motwani propose an approximate method: Find a point $p$ approximately to $q$ instead of closest to $q$, which is the __approximate nearest neighbor problem__.\n",
    "\n",
    "Indyk and Motawani found that s-NNS problem can be reduced to a new problem: __Point location in equal balls (PLEB)__. This reducition is achieved by a data structure called a ring-cover tree.\n",
    "\n",
    "They gave two algorithms to solve the PLEB problem, __Bucketing Method__ and __Locality-Sensitive Hashing__ respectively.\n",
    "\n",
    "The authors' idea of converting the NNS problem to ANN is very creative and the research method reducing one problem to another problem is practical and worth learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
